---
title: "PML exercise"
author: "Gianluca Emireni"
date: "February 9th 2015"
output: html_document
---


## Loading libraries
```{r, eval=T, }
library(randomForest)
library(caret)
```

## Reading datasets. 
From previous attempts, it turns out that there are different kind of _invalid_ data in training and testing set, in this case I found 'NA' and '#DIV/0!' values and identified as 'NA'. 
```{r, eval=F, }
train <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", sep=",", quote="\"", head=T, na.strings=c("NA","#DIV/0!"))
test <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", sep=",", quote="\"", head=T, na.strings=c("NA","#DIV/0!"))
```

## Data cleaning
All data cleaning procedures applied on the training dataset will be done also on the testing set.
Here I transform cvtd_timestamp from date to timestamp, then I remove the record counter (first column, not useful for our classification) for both datasets.
```{r, eval=F, }
train$cvtd_timestamp <- as.numeric(strptime(as.character(train$cvtd_timestamp), format = "%d/%m/%Y %H:%M"))
test$cvtd_timestamp <- as.numeric(strptime(as.character(test$cvtd_timestamp), format = "%d/%m/%Y %H:%M"))
train <- train[,-1]
test <- test[,-1]
```

Variables without valid values are excluded from the sets.
There are some variables with a lot of NA values. These variables are not useful for classification.
We can set a lower bound for the proportion of non-NA values for each predictor to be considered valid. So, if the proportion is below that limit, the variable should be eliminated for being uneffective in the estimate of the response variable/classification of observation.

Given an array x, this functions returns the proportion of NA.
```{r, eval=F, }
na.proportion <- function(x)
{
  prop <- sum(is.na(x))/length(x)
}
```

Applying the function to all the variables of training set
```{r, eval=F, }
train.nas <- apply(train, 2, na.proportion)
```
All the variables with more than 60% of NA values are excluded from training and testing set
```{r, eval=F, }
excluded.vars <- which(train.nas > 0.60)
```
The position of the variables in training and testing set is the same, except for the fact that
```{r, eval=F, }
# colnames(train)[160] = "classe"
# colnames(test)[160] = "problem_id"
```

New training and testing set are created, dropping the excluded variables from previous datasets.
```{r, eval=F, }
train2 <- train[,-excluded.vars]
test2 <- test[,-excluded.vars]
```
The total number of variables for each set is reduced to 59 (58 regressors).

Now we search for "near zero variance" variables, made of almost constant values that doesn't improve the quality of our classification.
```{r, eval=F, }
nzv <- nearZeroVar(train2, saveMetrics = T)
near_zero_var_cols <- rownames(nzv[nzv$nzv,])
near_zero_var_indexes <- which(colnames(train2)==rownames(nzv[nzv$nzv,]))
near_zero_var_indexes
```
# [1] "new_window"
The only remaining variable with "near zero variance" is the factor new_window, dropped from both datasets.
```{r, eval=F, }
train2 <- train2[,-near_zero_var_indexes]
test2 <- test2[,-near_zero_var_indexes]
```

Now we will look for highly correlated variables.
To do this, we need a data.frame entirely composed by numeric variables: 
```{r, eval=F, }
numeric_vars <- sapply(train2, is.numeric)
highly_correlated_cols <- findCorrelation(cor(train2[,numeric_vars]), cutoff = .90)
```

The function *findCorrelation* searches through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlation. This function suggest the elimination of some columns from training and testing datasets.
```{r, eval=F, }
excluded.colnames <- colnames(train2[,numeric_vars])[highly_correlated_cols]
excluded.vars <- which(colnames(train2) %in% excluded.colnames)
excluded.vars
```
New training and testing datasets are created dropping the high-correlated variables. 
The loss of information is limited in this case, and we gain in terms of model adaptability, reducing the risk of overfitting.
```{r, eval=F, }
train3 <- train2[, -excluded.vars]
test3 <- test2[, -excluded.vars]
```
Now each dataset have 51 variables.

The imputation of values for filling NA values is no more needed, now that we have removed the variables plagued by NA's.

## ML models specification and evaluation
We have 2 datasets: train and test set.
Training set is big enough to be divided in 2 parts: a real *training set* and a *validation set*, useful for out-of-sample evaluation of our classification. The test set remains the same.

I'm using 60% of observations for the _training set_ and the remaining 40% for the _validation set_.
```{r, eval=F, }
inTrain <- createDataPartition(y=train3$classe, p=0.6, list=FALSE)
training <- train3[inTrain, ]
validation <- train3[-inTrain, ]
```
Removing the "problem_id" variable, progressive variable not present in training set.
```{r, eval=F, }
testing <- test3[,-51]
```